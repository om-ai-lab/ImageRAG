{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e092af17-fe58-4532-bf5b-e4374ad27b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilun/anaconda3/envs/imagerag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import math\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12, use_dynamic=True):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    if use_dynamic:\n",
    "        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        print(\"Use Dynamic\")\n",
    "    else:\n",
    "        images = [image]\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def split_list(lst, n):\n",
    "    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n",
    "    chunk_size = math.ceil(len(lst) / n)  # integer division\n",
    "    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "\n",
    "def get_chunk(lst, n, k):\n",
    "    chunks = split_list(lst, n)\n",
    "    return chunks[k]\n",
    "\n",
    "\n",
    "def load_json_jsonl(file_path):\n",
    "    # Check the file extension and process accordingly\n",
    "    if file_path.endswith(\".jsonl\"):\n",
    "        # For JSONL files (line-delimited JSON)\n",
    "        data = [json.loads(line) for line in open(file_path, \"r\")]\n",
    "    elif file_path.endswith(\".json\"):\n",
    "        # For standard JSON files\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)  # Load the entire JSON file as a single object\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8659ce92-ec59-4416-a19b-9df0a65bd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_fpath = \"/media/zilun/fanxiang4t/GRSM/ImageRAG_git/data/eval/answer_mme-realworld-lite-8B-detection-fit-union-obj.jsonl\"\n",
    "model_path = \"/media/zilun/fanxiang4t/GRSM/ImageRAG_git/checkpoint/InternVL2_5-8B_lora_5param_dynamic_0-1000_obb2_fit_unionpatch_obj_merged\"\n",
    "image_root = \"/media/zilun/wd-161/datasets/MME-RealWorld/rs_subset/remote_sensing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c5c815-30fd-43a7-8ddf-09086f1765bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Question_id': 'perception/remote_sensing/position/2527', 'Image': 'dota_v2_dota_v2_dota_v2_P7036.png', 'Text': 'Where is the yellow crane in the picture?', 'Question Type': 'Multiple Choice', 'Answer choices': ['(A) In the upper right area of the picture', '(B) In the upper left area of the picture', '(C) In the lower right area of the picture', '(D) In the lower left area of the picture', \"(E) This image doesn't feature the position.\"], 'Ground truth': 'A', 'Category': 'position', 'Subtask': 'Remote Sensing', 'Task': 'Perception', 'target_of_interest': 'The yellow crane in the picture.', 'visual_cue': [727.0, 371.0, 1000.0, 715.0], 'output': '(A)'}\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "answers = load_json_jsonl(answer_fpath)\n",
    "example_answer = answers[0]\n",
    "print(example_answer)\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d32f416-16b5-4ca5-b90f-1f6b8cd2015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.60s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b54c72-ba6b-4023-9728-6b0e54b6dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_vlm_transform = build_transform(model.config.vision_config.image_size)\n",
    "generative_vlm = model\n",
    "generative_vlm_dynamic_preprocess = dynamic_preprocess\n",
    "generative_vlm_tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40050a6a-1343-45fd-823c-9e8719741cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_image_with_bbox(image_path, bboxes):\n",
    "    \"\"\"\n",
    "    Display an image with bounding boxes in Jupyter Notebook at its original size.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        bboxes (list): List of bounding boxes, each represented as (x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    # 读取图片\n",
    "    img = mpimg.imread(image_path)\n",
    "    \n",
    "    # 获取图像的原始尺寸\n",
    "    img_height, img_width = img.shape[:2]\n",
    "    \n",
    "    # 创建图形和轴，设置图形大小为图像的原始尺寸\n",
    "    dpi = 80  # 可以根据需要调整 DPI\n",
    "    figsize = (img_width / dpi, img_height / dpi)\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    \n",
    "    # 显示图片\n",
    "    ax.imshow(img)\n",
    "\n",
    "    if bboxes is not None:\n",
    "        # 绘制每个边界框\n",
    "        for bbox in bboxes:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            x1 = int(x1 / 1000 * img_width)\n",
    "            y1 = int(y1 / 1000 * img_height)\n",
    "            x2 = int(x2 / 1000 * img_width)\n",
    "            y2 = int(y2 / 1000 * img_height)\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=5, edgecolor='blue', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    # 移除轴标记\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 显示图形\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf8c4dfe-62ad-4c90-8662-26b513909e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_input(image_root, model_config, line, test_prompt, use_dynamic=False):\n",
    "    image_name = line[\"Image\"]\n",
    "    image_path = os.path.join(image_root, image_name)\n",
    "    # pixel_values = load_image(\n",
    "    #     image_path,\n",
    "    #     input_size=model_config.vision_config.image_size,\n",
    "    #     max_num=model_config.max_dynamic_patch,\n",
    "    #     use_dynamic=use_dynamic\n",
    "    # ).to(torch.bfloat16).cuda()\n",
    "\n",
    "    choices = line['Answer choices']\n",
    "    image_file = line[\"Image\"]\n",
    "    qs = line[\"Text\"]\n",
    "    choice_prompt = ' The choices are listed below: \\n'\n",
    "    for choice in choices:\n",
    "        choice_prompt += choice + \"\\n\"\n",
    "    qs += choice_prompt + test_prompt + '\\nThe best answer is:'\n",
    "\n",
    "    images, tile_num_list = [], []\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    w, h = image.size\n",
    "    image_anyres = generative_vlm_dynamic_preprocess(\n",
    "        image,\n",
    "        max_num=generative_vlm.config.max_dynamic_patch,\n",
    "        image_size=generative_vlm.config.vision_config.image_size,\n",
    "        use_thumbnail=generative_vlm.config.use_thumbnail,\n",
    "    )\n",
    "    images += image_anyres\n",
    "    pixel_values = [generative_vlm_transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values).to(torch.bfloat16).cuda()\n",
    "    tile_num_list.append(len(image_anyres))\n",
    "    # line[\"visual_cue\"][1] -= 300\n",
    "    predict_bbox = line[\"visual_cue\"]\n",
    "    \n",
    "    if predict_bbox:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        visual_cues = [predict_bbox]\n",
    "        for object_coord in visual_cues:\n",
    "            x1, y1, x2, y2 = object_coord\n",
    "            x1 = int(x1 / 1000 * w)\n",
    "            y1 = int(y1 / 1000 * h)\n",
    "            x2 = int(x2 / 1000 * w)\n",
    "            y2 = int(y2 / 1000 * h)\n",
    "            image_crop = image.crop((x1, y1, x2, y2))\n",
    "            images.append(image_crop)\n",
    "            tile_num_list.append(1)\n",
    "        pixel_values = [generative_vlm_transform(image) for image in images]\n",
    "        pixel_values = torch.stack(pixel_values).to(torch.bfloat16).cuda()\n",
    "\n",
    "        final_instruction = \"<image>\\n\"\n",
    "        final_instruction += \"Additional information:\\n\"\n",
    "        for i, bbox in enumerate(visual_cues):\n",
    "            final_instruction += \"Sub-patch {} at location <box>[[{:.2f}, {:.2f}, {:.2f}, {:.2f}]]</box>: <image>\\n\".format(\n",
    "                i + 1, *bbox)\n",
    "        final_instruction += \"Look at the image and answer the question based on the provided additional information (location of sub-patches). \\n\"\n",
    "        final_instruction += \"Question: \"\n",
    "        final_instruction += qs\n",
    "        num_patches_list = [pixel_values.size(0) - len(visual_cues), len(visual_cues)]\n",
    "        # response = generative_vlm.chat(\n",
    "        #     generative_vlm_tokenizer,\n",
    "        #     pixel_values,\n",
    "        #     final_instruction,\n",
    "        #     generative_vlm_generation_config,\n",
    "        #     num_patches_list=num_patches_list\n",
    "        # )\n",
    "    else:\n",
    "        final_instruction = question_with_test_template\n",
    "        num_patches_list = [pixel_values.size(0)]\n",
    "        # response = generative_vlm.chat(\n",
    "        #     generative_vlm_tokenizer,\n",
    "        #     pixel_values,\n",
    "        #     final_instruction,\n",
    "        #     generative_vlm_generation_config,\n",
    "        # )\n",
    "\n",
    "    return image_path, pixel_values, num_patches_list, final_instruction, predict_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04481e4d-2618-40bc-8049-993a3c0117e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, image_tensors, prompt, generation_param, num_patches_list):\n",
    "    prompts = [prompt]\n",
    "    generation_config = dict(\n",
    "        max_new_tokens=generation_param[\"max_new_tokens\"],\n",
    "        do_sample=True if generation_param[\"temperature\"] > 0 else False,\n",
    "        temperature=generation_param[\"temperature\"],\n",
    "        top_p=generation_param[\"top_p\"],\n",
    "        num_beams=generation_param[\"num_beams\"],\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        response = generative_vlm.chat(\n",
    "            generative_vlm_tokenizer,\n",
    "            image_tensors,\n",
    "            prompt,\n",
    "            generation_config,\n",
    "            num_patches_list=num_patches_list\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d67f014-0922-499f-af4b-0702900ff9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prompt = \"Select the best answer to the above multiple-choice question based on the image. Respond with only the letter (A, B, C, D, or E) of the correct option.\"\n",
    "test_prompt = \"Select the best answer to the above multiple-choice question based on the image.\"\n",
    "# test_prompt = \"<image>\\nPlease provide the bounding box coordinate of the region this sentence describes: \"\n",
    "sample = 50\n",
    "import random\n",
    "random.shuffle(answers)\n",
    "\n",
    "generation_param_dict = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.99,\n",
    "    \"num_beams\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7948bf36-df13-4cf0-b57a-b1e4bdb8fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for answer in answers[:sample]:\n",
    "#     image_path, image_tensors, num_patches_list, qs, predict_bbox = prepare_input(image_root, model.config, answer, test_prompt, use_dynamic=True)\n",
    "#     print(qs)\n",
    "#     print(image_tensors.shape, num_patches_list)\n",
    "#     print(predict_bbox)\n",
    "#     print(\"GT: {}\\n\".format(answer[\"Ground truth\"]))\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     response = inference(generative_vlm, generative_vlm_tokenizer, image_tensors, qs, generation_param_dict, num_patches_list)\n",
    "#     print(\"Ouput: {}\".format(response))\n",
    "#     show_image_with_bbox(image_path, [predict_bbox])\n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc616b5-8664-477f-b01a-01bb41bff134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12f6a3b-04e1-4b8a-8697-641bfc97ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_characters_regex(s, choices):\n",
    "    s = s.strip()\n",
    "    answer_prefixes = [\n",
    "        \"The best answer is\",\n",
    "        \"The correct answer is\",\n",
    "        \"The answer is\",\n",
    "        \"The answer\",\n",
    "        \"The best option is\"\n",
    "        \"The correct option is\",\n",
    "        \"Best answer:\"\n",
    "        \"Best option:\",\n",
    "    ]\n",
    "    for answer_prefix in answer_prefixes:\n",
    "        s = s.replace(answer_prefix, \"\")\n",
    "\n",
    "    if len(s.split()) > 10 and not re.search(\"[ABCDE]\", s):\n",
    "        return \"\"\n",
    "    matches = re.search(r'[ABCDE]', s)\n",
    "    if matches is None:\n",
    "        for choice in choices:\n",
    "            if s.lower() in choice.lower():\n",
    "                return choice[1]\n",
    "        return \"\"\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15024a87-afc2-40f1-a34d-79688fb73353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cc583-87d8-44ba-9ba0-e84e3064127b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
