{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e092af17-fe58-4532-bf5b-e4374ad27b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilun/anaconda3/envs/imagerag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import math\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12, use_dynamic=True):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    if use_dynamic:\n",
    "        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        print(\"Use Dynamic\")\n",
    "    else:\n",
    "        images = [image]\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def split_list(lst, n):\n",
    "    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n",
    "    chunk_size = math.ceil(len(lst) / n)  # integer division\n",
    "    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "\n",
    "def get_chunk(lst, n, k):\n",
    "    chunks = split_list(lst, n)\n",
    "    return chunks[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8659ce92-ec59-4416-a19b-9df0a65bd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_fpath = \"/media/zilun/fanxiang4t/GRSM/ImageRAG_git/codebase/inference/MME-RealWorld-RS/MME-RealWorld-Lite.json\"\n",
    "answer_fpath = \"/media/zilun/fanxiang4t/GRSM/ImageRAG_git/data/eval/answer_mme-realworld-lite-cot-explicit.jsonl\"\n",
    "model_path = \"/media/zilun/fanxiang4t/GRSM/ImageRAG_git/checkpoint/InternVL2_5-8B\"\n",
    "image_root = \"/media/zilun/wd-161/datasets/MME-RealWorld/rs_subset/remote_sensing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c5c815-30fd-43a7-8ddf-09086f1765bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Question_id': 'perception/remote_sensing/position/2527', 'Image': 'dota_v2_dota_v2_dota_v2_P7036.png', 'Text': 'Where is the yellow crane in the picture?', 'Question Type': 'Multiple Choice', 'Answer choices': ['(A) In the upper right area of the picture', '(B) In the upper left area of the picture', '(C) In the lower right area of the picture', '(D) In the lower left area of the picture', \"(E) This image doesn't feature the position.\"], 'Ground truth': 'A', 'Category': 'position', 'Subtask': 'Remote Sensing', 'Task': 'Perception', 'output': '(C) In the lower right area of the picture\\n\\nThe yellow crane can be seen in the lower right area of the picture. It is located near the intersection of the roads and is distinguishable by its bright yellow color and crane-like structure.'}\n",
      "150 150\n"
     ]
    }
   ],
   "source": [
    "answers = [json.loads(line) for line in open(answer_fpath, \"r\")]\n",
    "with open(question_fpath, 'r') as file:\n",
    "    questions = json.load(file)\n",
    "questions = [question for question in questions if question[\"Subtask\"] == \"Remote Sensing\"]\n",
    "questions = get_chunk(questions, 1, 0)\n",
    "example_answer = answers[0]\n",
    "print(example_answer)\n",
    "print(len(questions), len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d32f416-16b5-4ca5-b90f-1f6b8cd2015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.59s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40050a6a-1343-45fd-823c-9e8719741cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_image_with_bbox(image_path, bboxes):\n",
    "    \"\"\"\n",
    "    Display an image with bounding boxes in Jupyter Notebook at its original size.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        bboxes (list): List of bounding boxes, each represented as (x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    # 读取图片\n",
    "    img = mpimg.imread(image_path)\n",
    "    \n",
    "    # 获取图像的原始尺寸\n",
    "    img_height, img_width = img.shape[:2]\n",
    "    \n",
    "    # 创建图形和轴，设置图形大小为图像的原始尺寸\n",
    "    dpi = 80  # 可以根据需要调整 DPI\n",
    "    figsize = (img_width / dpi, img_height / dpi)\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    \n",
    "    # 显示图片\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # 绘制每个边界框\n",
    "    for bbox in bboxes:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        x1 = int(x1 / 1000 * img_width)\n",
    "        y1 = int(y1 / 1000 * img_height)\n",
    "        x2 = int(x2 / 1000 * img_width)\n",
    "        y2 = int(y2 / 1000 * img_height)\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=5, edgecolor='blue', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # 移除轴标记\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 显示图形\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf8c4dfe-62ad-4c90-8662-26b513909e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(image_root, model_config, line, test_prompt, use_dynamic=False):\n",
    "    image_name = line[\"Image\"]\n",
    "    image_path = os.path.join(image_root, image_name)\n",
    "    pixel_values = load_image(\n",
    "        image_path,\n",
    "        input_size=model_config.vision_config.image_size,\n",
    "        max_num=model_config.max_dynamic_patch,\n",
    "        use_dynamic=use_dynamic\n",
    "    ).to(torch.bfloat16).cuda()\n",
    "\n",
    "    choices = line['Answer choices']\n",
    "    image_file = line[\"Image\"]\n",
    "    qs = line[\"Text\"]\n",
    "    # choice_prompt = ' The choices are listed below: \\n'\n",
    "    # for choice in choices:\n",
    "    #     choice_prompt += choice + \"\\n\"\n",
    "    # qs += choice_prompt + test_prompt + '\\nThe best answer is:'\n",
    "    # final_instruction = qs\n",
    "    final_instruction = test_prompt + \"<ref>\" + \"yellow crane\" + \"</ref>\"\n",
    "    return image_path, pixel_values, [pixel_values.size(0)], final_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04481e4d-2618-40bc-8049-993a3c0117e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, image_tensors, prompt, generation_param):\n",
    "    num_patches_list = [image_tensors.shape[0]]\n",
    "    prompts = [prompt]\n",
    "    generation_config = dict(\n",
    "        max_new_tokens=generation_param[\"max_new_tokens\"],\n",
    "        do_sample=True if generation_param[\"temperature\"] > 0 else False,\n",
    "        temperature=generation_param[\"temperature\"],\n",
    "        top_p=generation_param[\"top_p\"],\n",
    "        num_beams=generation_param[\"num_beams\"],\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        responses = model.batch_chat(\n",
    "            tokenizer,\n",
    "            image_tensors,\n",
    "            num_patches_list=num_patches_list,\n",
    "            questions=prompts,\n",
    "            generation_config=generation_config\n",
    "    )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d67f014-0922-499f-af4b-0702900ff9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Dynamic\n"
     ]
    }
   ],
   "source": [
    "# test_prompt = \"Select the best answer to the above multiple-choice question based on the image. Respond with only the letter (A, B, C, D, or E) of the correct option.\"\n",
    "test_prompt = \"<image>\\nPlease provide the bounding box coordinate of the region this sentence describes: \"\n",
    "image_path, image_tensors, num_patches_list, qs = prepare_input(image_root, model.config, example_answer, test_prompt, use_dynamic=True)\n",
    "# image_path = \"/home/zilun/Desktop/953457412.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dcfd9cc-1162-495f-b430-ee4f5292e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7948bf36-df13-4cf0-b57a-b1e4bdb8fd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 3, 448, 448])\n",
      "[7]\n",
      "<image>\n",
      "Please provide the bounding box coordinate of the region this sentence describes: <ref>yellow crane</ref>\n",
      "GT: A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilun/anaconda3/envs/imagerag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouput: yellow crane[[721, 315, 1000, 715]]\n"
     ]
    }
   ],
   "source": [
    "print(image_tensors.shape)\n",
    "print(num_patches_list)\n",
    "print(qs)\n",
    "print(\"GT: {}\\n\".format(example_answer[\"Ground truth\"]))\n",
    "\n",
    "generation_param_dict = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "}\n",
    "\n",
    "responses = inference(model, tokenizer, image_tensors, qs, generation_param_dict)\n",
    "print(\"Ouput: {}\".format(responses[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc616b5-8664-477f-b01a-01bb41bff134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERN = re.compile(r'\\[*\\[(.*?),(.*?),(.*?),(.*?)\\]\\]*')\n",
    "# predict_bbox = re.findall(PATTERN, responses[0])\n",
    "# try:\n",
    "#     predict_bbox = (float(predict_bbox[0][0]), float(predict_bbox[0][1]), float(predict_bbox[0][2]),\n",
    "#                     float(predict_bbox[0][3]))\n",
    "# except:\n",
    "#     predict_bbox = None\n",
    "# bboxes = [predict_bbox]\n",
    "# print(predict_bbox)\n",
    "# show_image_with_bbox(image_path, bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12f6a3b-04e1-4b8a-8697-641bfc97ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_characters_regex(s, choices):\n",
    "    s = s.strip()\n",
    "    answer_prefixes = [\n",
    "        \"The best answer is\",\n",
    "        \"The correct answer is\",\n",
    "        \"The answer is\",\n",
    "        \"The answer\",\n",
    "        \"The best option is\"\n",
    "        \"The correct option is\",\n",
    "        \"Best answer:\"\n",
    "        \"Best option:\",\n",
    "    ]\n",
    "    for answer_prefix in answer_prefixes:\n",
    "        s = s.replace(answer_prefix, \"\")\n",
    "\n",
    "    if len(s.split()) > 10 and not re.search(\"[ABCDE]\", s):\n",
    "        return \"\"\n",
    "    matches = re.search(r'[ABCDE]', s)\n",
    "    if matches is None:\n",
    "        for choice in choices:\n",
    "            if s.lower() in choice.lower():\n",
    "                return choice[1]\n",
    "        return \"\"\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15024a87-afc2-40f1-a34d-79688fb73353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cc583-87d8-44ba-9ba0-e84e3064127b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
