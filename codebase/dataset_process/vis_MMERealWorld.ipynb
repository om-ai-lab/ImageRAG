{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e092af17-fe58-4532-bf5b-e4374ad27b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilun/anaconda3/envs/imagerag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import math\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import sys\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12, use_dynamic=True):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    if use_dynamic:\n",
    "        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        print(\"Use Dynamic\")\n",
    "    else:\n",
    "        images = [image]\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def split_list(lst, n):\n",
    "    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n",
    "    chunk_size = math.ceil(len(lst) / n)  # integer division\n",
    "    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "\n",
    "def get_chunk(lst, n, k):\n",
    "    chunks = split_list(lst, n)\n",
    "    return chunks[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8659ce92-ec59-4416-a19b-9df0a65bd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model-path\", type=str, default=\"/media/zilun/fanxiang4t/GRSM/ImageRAG_git/checkpoint/InternVL2_5-4B\")\n",
    "# parser.add_argument(\"--model-path\", type=str, default=\"/media/zilun/fanxiang4t/GRSM/ImageRAG_git/checkpoint/InternVL2_5-8B\")\n",
    "parser.add_argument(\"--model-base\", type=str, default=None)\n",
    "parser.add_argument(\"--image-folder\", type=str, default=\"/media/zilun/wd-161/datasets/MME-RealWorld/rs_subset\")\n",
    "parser.add_argument(\"--question-file\", type=str, default=\"/media/zilun/fanxiang4t/GRSM/ImageRAG_git/data/eval/MME_RealWorld.json\")\n",
    "parser.add_argument(\"--answers-file\", type=str, default=\"/media/zilun/fanxiang4t/GRSM/ImageRAG_git/data/answer_mme-realworld-rs-test.jsonl\")\n",
    "parser.add_argument(\"--conv-mode\", type=str, default=\"internvl\")\n",
    "parser.add_argument(\"--num-chunks\", type=int, default=1)\n",
    "parser.add_argument(\"--chunk-idx\", type=int, default=0)\n",
    "parser.add_argument(\"--temperature\", type=float, default=0.9)\n",
    "parser.add_argument(\"--top_p\", type=float, default=None)\n",
    "parser.add_argument(\"--num_beams\", type=int, default=1)\n",
    "parser.add_argument(\"--max_new_tokens\", type=int, default=64)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "\n",
    "parser.add_argument(\"--use-qlora\", type=bool, default=False)\n",
    "parser.add_argument(\"--qlora-path\", type=str, default=\"\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--test-prompt\",\n",
    "    type=str,\n",
    "    default=\"Select the best answer to the above multiple-choice question based on the image. Respond with only the letter (A, B, C, D, or E) of the correct option.\",\n",
    ")\n",
    "\n",
    "args = parser.parse_known_args(sys.argv[1:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e358fb-4932-4222-bdcb-c34c64eed446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(model_path='/media/zilun/fanxiang4t/GRSM/ImageRAG_git/checkpoint/InternVL2_5-4B', model_base=None, image_folder='/media/zilun/wd-161/datasets/MME-RealWorld/rs_subset', question_file='/media/zilun/fanxiang4t/GRSM/ImageRAG_git/data/eval/MME_RealWorld.json', answers_file='/media/zilun/fanxiang4t/GRSM/ImageRAG_git/data/answer_mme-realworld-rs-test.jsonl', conv_mode='internvl', num_chunks=1, chunk_idx=0, temperature=0.9, top_p=None, num_beams=1, max_new_tokens=64, batch_size=2, use_qlora=False, qlora_path='', test_prompt='Select the best answer to the above multiple-choice question based on the image. Respond with only the letter (A, B, C, D, or E) of the correct option.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d32f416-16b5-4ca5-b90f-1f6b8cd2015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  8.61it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = AutoModel.from_pretrained(\n",
    "    args.model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_path,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "with open(args.question_file, 'r') as file:\n",
    "    questions = json.load(file)\n",
    "questions = [question for question in questions if question[\"Subtask\"] == \"Remote Sensing\"]\n",
    "questions = get_chunk(questions, args.num_chunks, args.chunk_idx)\n",
    "answers_file = os.path.expanduser(args.answers_file)\n",
    "os.makedirs(os.path.dirname(answers_file), exist_ok=True)\n",
    "ans_file = open(answers_file, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c449660-c96a-4f73-8d5a-5b9f4d73f339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Question_id': 'perception/remote_sensing/color/0001',\n",
       "  'Image': 'remote_sensing/03553_Toronto.png',\n",
       "  'Text': 'What color is the roof of the square building in the lower right area of the picture?',\n",
       "  'Question Type': 'Multiple Choice',\n",
       "  'Answer choices': ['(A) Yellow',\n",
       "   '(B) Blue',\n",
       "   '(C) Gray',\n",
       "   '(D) White',\n",
       "   '(E) The image does not feature the color.'],\n",
       "  'Ground truth': 'D',\n",
       "  'Category': 'color',\n",
       "  'Subtask': 'Remote Sensing',\n",
       "  'Task': 'Perception'},\n",
       " {'Question_id': 'perception/remote_sensing/color/0004',\n",
       "  'Image': 'remote_sensing/03555_Toronto.png',\n",
       "  'Text': 'What color is the roof of the square building on the middle right area of the picture?',\n",
       "  'Question Type': 'Multiple Choice',\n",
       "  'Answer choices': ['(A) Black',\n",
       "   '(B) White',\n",
       "   '(C) Blue',\n",
       "   '(D) Green',\n",
       "   \"(E) This image doesn't feature the color.\"],\n",
       "  'Ground truth': 'B',\n",
       "  'Category': 'color',\n",
       "  'Subtask': 'Remote Sensing',\n",
       "  'Task': 'Perception'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7243dc0-e2c2-4222-801c-bae4e1124108",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"Screenshot from 2025-01-07 19-55-00.png\"\n",
    "line = questions[0]\n",
    "image_file = line[\"Image\"]\n",
    "# image_path = os.path.join(args.image_folder, image_file)\n",
    "# question = \"Identify the coordinates of roof of the square building in the lower right area of the picture\"\n",
    "# line[\"Text\"] = question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03fd589d-28fc-4c9a-b573-6a8c374982da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(image_path, model_config, line):\n",
    "    \n",
    "    pixel_values = load_image(\n",
    "        image_path,\n",
    "        input_size=model_config.vision_config.image_size,\n",
    "        max_num=model_config.max_dynamic_patch,\n",
    "        use_dynamic=True\n",
    "    ).to(torch.bfloat16).cuda()\n",
    "\n",
    "    choices = line['Answer choices']\n",
    "    image_file = line[\"Image\"]\n",
    "    qs = line[\"Text\"]\n",
    "    choice_prompt = ' The choices are listed below: \\n'\n",
    "    for choice in choices:\n",
    "        choice_prompt += choice + \"\\n\"\n",
    "    qs += choice_prompt + args.test_prompt + '\\nThe best answer is:'\n",
    "    \n",
    "    return pixel_values, [pixel_values.size(0)], [qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855925ac-5e60-49d3-8d0d-d63907323ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Dynamic\n",
      "torch.Size([10, 3, 448, 448])\n",
      "[10]\n",
      "What color is the roof of the square building in the lower right area of the picture? The choices are listed below: \n",
      "(A) Yellow\n",
      "(B) Blue\n",
      "(C) Gray\n",
      "(D) White\n",
      "(E) The image does not feature the color.\n",
      "Select the best answer to the above multiple-choice question based on the image. Respond with only the letter (A, B, C, D, or E) of the correct option.\n",
      "The best answer is:\n"
     ]
    }
   ],
   "source": [
    "image_tensors, num_patches_list, qs = prepare_input(image_path, model.config, line)\n",
    "print(image_tensors.shape)\n",
    "print(num_patches_list)\n",
    "print(qs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40050a6a-1343-45fd-823c-9e8719741cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_image_with_bbox(image_path, bboxes):\n",
    "    \"\"\"\n",
    "    Display an image with bounding boxes in Jupyter Notebook at its original size.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        bboxes (list): List of bounding boxes, each represented as (x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    # 读取图片\n",
    "    img = mpimg.imread(image_path)\n",
    "    \n",
    "    # 获取图像的原始尺寸\n",
    "    img_height, img_width = img.shape[:2]\n",
    "    \n",
    "    # 创建图形和轴，设置图形大小为图像的原始尺寸\n",
    "    dpi = 80  # 可以根据需要调整 DPI\n",
    "    figsize = (img_width / dpi, img_height / dpi)\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    \n",
    "    # 显示图片\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # 绘制每个边界框\n",
    "    for bbox in bboxes:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # 移除轴标记\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 显示图形\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4eb894-d123-44bb-bf54-f2091f177cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bboxes = [(50, 50, 150, 150), (100, 100, 200, 200)]  # 替换为实际的边界框坐标\n",
    "# show_image_with_bbox(image_path, bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04481e4d-2618-40bc-8049-993a3c0117e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, image_tensors, num_patches_list, prompts):\n",
    "    generation_config = dict(\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        do_sample=True if args.temperature > 0 else False,\n",
    "        temperature=args.temperature,\n",
    "        top_p=args.top_p,\n",
    "        num_beams=args.num_beams,\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        responses = model.batch_chat(\n",
    "            tokenizer,\n",
    "            image_tensors,\n",
    "            num_patches_list=num_patches_list,\n",
    "            questions=prompts,\n",
    "            generation_config=generation_config\n",
    "    )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d67f014-0922-499f-af4b-0702900ff9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "responses = inference(model, tokenizer, image_tensors, num_patches_list, qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e690ac-466b-4fc5-a012-4836e908f9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7948bf36-df13-4cf0-b57a-b1e4bdb8fd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color is the roof of the square building in the lower right area of the picture? The choices are listed below: \n",
      "(A) Yellow\n",
      "(B) Blue\n",
      "(C) Gray\n",
      "(D) White\n",
      "(E) The image does not feature the color.\n",
      "Select the best answer to the above multiple-choice question based on the image. Respond with only the letter (A, B, C, D, or E) of the correct option.\n",
      "The best answer is:\n"
     ]
    }
   ],
   "source": [
    "print(qs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d1f78e7-471f-4572-b3b3-252f76b1da8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patches_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ff42ed-7158-4eb3-bb34-de4c1a8cba0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 448, 448])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc616b5-8664-477f-b01a-01bb41bff134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
