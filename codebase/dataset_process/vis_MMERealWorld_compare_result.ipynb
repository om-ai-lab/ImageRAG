{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e092af17-fe58-4532-bf5b-e4374ad27b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilun/anaconda3/envs/imagerag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import math\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12, use_dynamic=True):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    if use_dynamic:\n",
    "        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        print(\"Use Dynamic\")\n",
    "    else:\n",
    "        images = [image]\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def split_list(lst, n):\n",
    "    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n",
    "    chunk_size = math.ceil(len(lst) / n)  # integer division\n",
    "    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "\n",
    "def get_chunk(lst, n, k):\n",
    "    chunks = split_list(lst, n)\n",
    "    return chunks[k]\n",
    "\n",
    "\n",
    "def load_json_jsonl(file_path):\n",
    "    # Check the file extension and process accordingly\n",
    "    if file_path.endswith(\".jsonl\"):\n",
    "        # For JSONL files (line-delimited JSON)\n",
    "        data = [json.loads(line) for line in open(file_path, \"r\")]\n",
    "    elif file_path.endswith(\".json\"):\n",
    "        # For standard JSON files\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)  # Load the entire JSON file as a single object\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def show_image_with_bbox(image_path, bboxes):\n",
    "    \"\"\"\n",
    "    Display an image with bounding boxes in Jupyter Notebook at its original size.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        bboxes (list): List of bounding boxes, each represented as (x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    # 读取图片\n",
    "    img = mpimg.imread(image_path)\n",
    "    \n",
    "    # 获取图像的原始尺寸\n",
    "    img_height, img_width = img.shape[:2]\n",
    "    \n",
    "    # 创建图形和轴，设置图形大小为图像的原始尺寸\n",
    "    dpi = 80  # 可以根据需要调整 DPI\n",
    "    figsize = (img_width / dpi, img_height / dpi)\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    \n",
    "    # 显示图片\n",
    "    ax.imshow(img)\n",
    "\n",
    "    if bboxes is not None:\n",
    "        # 绘制每个边界框\n",
    "        for bbox in bboxes:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            x1 = int(x1 / 1000 * img_width)\n",
    "            y1 = int(y1 / 1000 * img_height)\n",
    "            x2 = int(x2 / 1000 * img_width)\n",
    "            y2 = int(y2 / 1000 * img_height)\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=5, edgecolor='blue', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    # 移除轴标记\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 显示图形\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def extract_characters_regex(s, choices):\n",
    "    s = s.strip()\n",
    "    answer_prefixes = [\n",
    "        \"The best answer is\",\n",
    "        \"The correct answer is\",\n",
    "        \"The answer is\",\n",
    "        \"The answer\",\n",
    "        \"The best option is\"\n",
    "        \"The correct option is\",\n",
    "        \"Best answer:\"\n",
    "        \"Best option:\",\n",
    "    ]\n",
    "    for answer_prefix in answer_prefixes:\n",
    "        s = s.replace(answer_prefix, \"\")\n",
    "\n",
    "    if len(s.split()) > 10 and not re.search(\"[ABCDE]\", s):\n",
    "        return \"\"\n",
    "    matches = re.search(r'[ABCDE]', s)\n",
    "    if matches is None:\n",
    "        for choice in choices:\n",
    "            if s.lower() in choice.lower():\n",
    "                return choice[1]\n",
    "        return \"\"\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8659ce92-ec59-4416-a19b-9df0a65bd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_answer_fpath = \"/media/zilun/fanxiang4t/GRSM/ImageRAG_git/codebase/inference/MME-RealWorld-RS/answer_baseline-mme-lite_8B_detection-gt.jsonl\"\n",
    "imagerag_answer_fpath = \"/media/zilun/fanxiang4t/GRSM/ImageRAG_git/codebase/inference/MME-RealWorld-RS/answer-corrected_zoom4klora32-mme-lite_8B_detection-gt-baseline-cot.jsonl\"\n",
    "image_root = \"/media/zilun/wd-161/datasets/MME-RealWorld/rs_subset/remote_sensing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94c5c815-30fd-43a7-8ddf-09086f1765bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Question_id': 'perception/remote_sensing/position/2527',\n",
       " 'Image': 'dota_v2_dota_v2_dota_v2_P7036.png',\n",
       " 'Text': 'Where is the yellow crane in the picture?',\n",
       " 'Question Type': 'Multiple Choice',\n",
       " 'Answer choices': ['(A) In the upper right area of the picture',\n",
       "  '(B) In the upper left area of the picture',\n",
       "  '(C) In the lower right area of the picture',\n",
       "  '(D) In the lower left area of the picture',\n",
       "  \"(E) This image doesn't feature the position.\"],\n",
       " 'Ground truth': 'A',\n",
       " 'Category': 'position',\n",
       " 'Subtask': 'Remote Sensing',\n",
       " 'Task': 'Perception',\n",
       " 'gt_toi': [6592, 970, 6970, 1569],\n",
       " 'img_size': [7360, 4912],\n",
       " 'output': \"To determine the position of the yellow crane in the picture, we need to carefully examine the aerial view of the city. The crane is typically a tall structure, often used in construction, and is usually quite distinct due to its color and shape.\\n\\n1. **Upper Right Area (A)**: This area contains several large buildings, but there is no visible yellow crane.\\n2. **Upper Left Area (B)**: This area has a mix of buildings and roads. There is a noticeable yellow crane in this section, which stands out due to its color and height.\\n3. **Lower Right Area (C)**: This area is densely packed with buildings and roads, but no yellow crane is visible.\\n4. **Lower Left Area (D)**: This area also contains buildings and roads, but no yellow crane is visible.\\n5. **This Image Doesn't Feature the Position (E)**: This option is incorrect because the yellow crane is present in the image.\\n\\nBased on the analysis, the yellow crane is located in the upper left area of the picture.\\n\\nThe best answer is: **(B) In the upper left area of the picture**.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_answers = load_json_jsonl(baseline_answer_fpath)\n",
    "imagerag_answers = load_json_jsonl(imagerag_answer_fpath)\n",
    "print(len(baseline_answers), len(imagerag_answers))\n",
    "imagerag_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd7ae51-69b3-4c4f-beac-7f3be4c45bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/zilun/fanxiang4t/GRSM/ImageRAG_git/codebase/dataset_process\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7215d92b-a0ce-479e-9bdb-116da043332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(image_root, line, mode=\"baseline\"):\n",
    "    # print(line)\n",
    "    test_prompt = \"Select the best answer to the above multiple-choice question based on the image. Respond with only the letter (A, B, C, D, or E) of the correct option.\"\n",
    "    image_name = line[\"Image\"]\n",
    "    image_path = os.path.join(image_root, image_name)\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    w, h = image.size \n",
    "    choices = line['Answer choices']\n",
    "    qs = line[\"Text\"]\n",
    "    choice_prompt = ' The choices are listed below: \\n'\n",
    "    for choice in choices:\n",
    "        choice_prompt += choice + \"\\n\"\n",
    "    qs += choice_prompt + test_prompt + '\\nThe best answer is:'\n",
    "    output = line[\"output\"]\n",
    "    # 'Ground truth': 'A', 'Category': 'position'\n",
    "    gt = line[\"Ground truth\"]\n",
    "    category = line[\"Category\"]\n",
    "    \n",
    "    if mode == \"imagerag\":\n",
    "        try:\n",
    "            predict_bbox = line[\"visual_cue\"]\n",
    "            predict_bbox = [int(predict_bbox[0]) / w * 1000, int(predict_bbox[1]) / h * 1000, int(predict_bbox[2]) / w * 1000, int(predict_bbox[3]) / h * 1000]\n",
    "            qs = line[\"final_instruction\"]\n",
    "        except:\n",
    "            predict_bbox = [0, 0, 1000, 1000]\n",
    "        print(predict_bbox)\n",
    "        \n",
    "        # toi = line[\"target_of_interest\"]\n",
    "        return image_name, image_path, qs, output, gt, category, [predict_bbox]\n",
    "    else:\n",
    "        return image_name, image_path, qs, output, gt, category\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40050a6a-1343-45fd-823c-9e8719741cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = start + 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7948bf36-df13-4cf0-b57a-b1e4bdb8fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(start, end):\n",
    "#     baseline_answer = baseline_answers[i]\n",
    "#     imagerag_answer = imagerag_answers[i]\n",
    "#     baseline_img_name, baseline_img_path, baseline_qs, baseline_output, baseline_gt, baseline_category = prepare_input(image_root, baseline_answer, mode=\"baseline\")\n",
    "#     imagerag_img_name, imagerag_img_path, imagerag_qs, imagerag_output, imagerag_gt, imagerag_category, imgrag_boxes = prepare_input(image_root, imagerag_answer, mode=\"imagerag\")\n",
    "#     assert baseline_img_name == imagerag_img_name\n",
    "#     # assert baseline_qs == imagerag_qs\n",
    "#     # imgrag_boxes = [[20.0, 949.1666666666667, 39.16666666666667, 999.6666666666667]]\n",
    "#     show_image_with_bbox(imagerag_img_path, imgrag_boxes)\n",
    "#     print(\"Index: {}\\nQuestion: {}\\nCategory: {}\\nGT: {}\\nBaseline Output: {}\\nImageRAG Output: {}\\nImageRAG Bbox: {}\\n\".format(i+1, imagerag_qs, imagerag_category, imagerag_gt, baseline_output, imagerag_output, imgrag_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc616b5-8664-477f-b01a-01bb41bff134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f6a3b-04e1-4b8a-8697-641bfc97ab5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15024a87-afc2-40f1-a34d-79688fb73353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cc583-87d8-44ba-9ba0-e84e3064127b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
