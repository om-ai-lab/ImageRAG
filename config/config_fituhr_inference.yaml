task_name:
  "fit_uhr"

input_uhr_image_path:
  "/media/zilun/fanxiang4t/GRSM/IRAG/image/paper_teaser.png"

input_image_dir:
  "/media/zilun/fanxiang4t/GRSM/evaluation_dataset/VQA_VG/FIT/FIT-RS/FIT-RS_Instruction/FIT-RS_Img/imgv2_split_512_100_vaild"

patch_saving_dir:
  "patch_save_dir"

model_input_image_size:
  336

fast_path_T:
  70

paraphrase_model:
    model_path:
        "/media/zilun/wd-161/hf_download/Qwen2.5-3B-Instruct"
    generation_config:
        temperature:
            1.0
        top_p:
            0.99
        max_tokens:
            512
        timeout:
            60

kw_model:
#    model_path:
#        "/media/zilun/wd-161/hf_download/all-MiniLM-L6-v2"
#    generation_config:
#        temperature:
#            0.99
#        top_p:
#            0.95
#        max_tokens:
#            512
#        timeout:
#            60

text_expansion_model:

fast_vlm_model:
    model_name:
      "skysensegpt"
    model_path:
      "/media/zilun/fanxiang4t/GRSM/SkySenseGPT/result/skysensegpt-fullft-1e6"
    num_chunks:
      1
    chunk_idx:
      0
    temperature:
      0.2
    top_p:
      None

llmvqa_model:
  model_path:
      "/media/zilun/fanxiang4t/GRSM/IRAG/imageRAG/ckpt/ckpt_llava-onevision-qwen2-0.5b-ov"

vector_database:
    meta_pkl_path:
        "/media/zilun/mx500/ImageRAG_databas'
        ]e/cropped_img/meta_df.pkl"
    img_dir:
      "/media/zilun/mx500/ImageRAG_database/cropped_img"
    text_vector_database_dir:
      "/media/zilun/fanxiang4t/GRSM/IRAG/imageRAG/codebase/vector_database/text_vector_database"
    image_vector_database_dir:
      "/media/zilun/fanxiang4t/GRSM/IRAG/imageRAG/codebase/vector_database/image_vector_database"
    mm_vector_database_dir:
      "/media/zilun/fanxiang4t/GRSM/IRAG/imageRAG/codebase/vector_database/mm_vector_database"

question_file_path:
  "/media/zilun/fanxiang4t/GRSM/ImageRAG_git/data/eval/test_FITRS_complex_comprehension_eval_boxsubset.jsonl"

answers_file_path:
  "/media/zilun/fanxiang4t/GRSM/ImageRAG_git/data/eval/test_FITRS_complex_comprehension_eval_boxsubset_eval.jsonl"

batch_size:
  1

conv_mode:
  "llava_v1"